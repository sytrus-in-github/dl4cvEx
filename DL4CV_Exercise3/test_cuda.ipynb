{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named common",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-12b295490a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#from test_torch import TestTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTestCase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_gpu_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreeze_rng_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mHAS_CUDA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named common"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tempfile\n",
    "import unittest\n",
    "from itertools import repeat\n",
    "\n",
    "import torch\n",
    "import torch.cuda\n",
    "import torch.cuda.comm as comm\n",
    "\n",
    "from test_torch import TestTorch\n",
    "from common import TestCase, get_gpu_type, to_gpu, freeze_rng_state, run_tests\n",
    "\n",
    "HAS_CUDA = True\n",
    "if not torch.cuda.is_available():\n",
    "    print('CUDA not available, skipping tests')\n",
    "    TestCase = object  # noqa: F811\n",
    "    HAS_CUDA = False\n",
    "\n",
    "\n",
    "def is_floating(t):\n",
    "    return type(t) in [torch.FloatTensor, torch.DoubleTensor,\n",
    "                       torch.cuda.FloatTensor, torch.cuda.DoubleTensor]\n",
    "\n",
    "types = [\n",
    "    torch.FloatTensor,\n",
    "    torch.DoubleTensor,\n",
    "    torch.LongTensor,\n",
    "    torch.IntTensor,\n",
    "    torch.ShortTensor,\n",
    "    torch.CharTensor,\n",
    "    torch.ByteTensor,\n",
    "]\n",
    "\n",
    "float_types = [\n",
    "    torch.FloatTensor,\n",
    "    torch.DoubleTensor\n",
    "]  # TODO: add half...\n",
    "\n",
    "\n",
    "def number(floating, integer, t):\n",
    "    name = type(t).__name__\n",
    "    if 'Double' in name or 'Float' in name or 'Half' in name:\n",
    "        return floating\n",
    "    else:\n",
    "        return integer\n",
    "# TODO: check HalfTensor\n",
    "\n",
    "S = 10\n",
    "M = 50\n",
    "\n",
    "\n",
    "def make_tensor(t, *sizes):\n",
    "    return t(*sizes).copy_(torch.randn(*sizes))\n",
    "\n",
    "\n",
    "def small_2d(t):\n",
    "    return make_tensor(t, S, S)\n",
    "\n",
    "\n",
    "def small_2d_scaled(t, scale=10):\n",
    "    return make_tensor(t, S, S).mul(scale)\n",
    "\n",
    "\n",
    "def small_2d_oneish(t):\n",
    "    if is_floating(t):\n",
    "        return make_tensor(t, S, S).clamp(min=0.99, max=1.01)\n",
    "    else:\n",
    "        return t(S, S).fill_(1)\n",
    "\n",
    "\n",
    "def small_3d(t):\n",
    "    return make_tensor(t, S, S, S)\n",
    "\n",
    "\n",
    "def medium_1d(t):\n",
    "    return make_tensor(t, M)\n",
    "\n",
    "\n",
    "def medium_2d(t):\n",
    "    return make_tensor(t, M, M)\n",
    "\n",
    "\n",
    "def medium_2d_scaled(t, scale=10):\n",
    "    return make_tensor(t, M, M).mul(scale)\n",
    "\n",
    "\n",
    "def small_3d_ones(t):\n",
    "    return t(S, S, S).copy_(torch.ones(S, S, S))\n",
    "\n",
    "\n",
    "def small_3d_positive(t):\n",
    "    min_val = 1e-3 if is_floating(t) else 2\n",
    "    return make_tensor(t, S, S, S).clamp_(min_val, 120)\n",
    "\n",
    "\n",
    "def small_3d_unique(t):\n",
    "    return t(S, S, S).copy_(torch.arange(1, S * S * S + 1))\n",
    "\n",
    "\n",
    "def small_1d_lapack(t):\n",
    "    return t(1, 3).copy_(torch.arange(1, 4).view(3))\n",
    "\n",
    "\n",
    "def small_2d_lapack(t):\n",
    "    return t(3, 3).copy_(torch.arange(1, 10).view(3, 3))\n",
    "\n",
    "\n",
    "def small_2d_lapack_skinny(t):\n",
    "    return t(3, 4).copy_(torch.arange(1, 13).view(3, 4))\n",
    "\n",
    "\n",
    "def small_2d_lapack_fat(t):\n",
    "    return t(4, 3).copy_(torch.arange(1, 13).view(4, 3))\n",
    "\n",
    "\n",
    "def large_2d_lapack(t):\n",
    "    return t(1000, 1000).normal_()\n",
    "\n",
    "\n",
    "def new_t(*sizes):\n",
    "    def tmp(t):\n",
    "        return t(*sizes).copy_(torch.randn(*sizes))\n",
    "    return tmp\n",
    "\n",
    "tests = [\n",
    "    ('add', small_3d, lambda t: [number(3.14, 3, t)]),\n",
    "    ('add', small_3d, lambda t: [small_3d_positive(t)], 'tensor'),\n",
    "    ('add', small_3d, lambda t: [number(0.2, 2, t), small_3d_positive(t)], 'scalar_tensor'),\n",
    "    ('sub', small_3d, lambda t: [number(3.14, 3, t)],),\n",
    "    ('sub', small_3d, lambda t: [small_3d_positive(t)], 'tensor'),\n",
    "    ('mul', small_3d, lambda t: [number(3.14, 3, t)],),\n",
    "    ('mul', small_3d, lambda t: [small_3d_positive(t)], 'tensor'),\n",
    "    ('div', small_3d, lambda t: [number(3.14, 3, t)],),\n",
    "    ('div', small_3d, lambda t: [small_3d_positive(t)], 'tensor'),\n",
    "    ('pow', small_3d, lambda t: [number(3.14, 3, t)], None, float_types),\n",
    "    ('pow', small_3d, lambda t: [small_3d(t).abs_()], 'tensor', float_types),\n",
    "    ('addbmm', small_2d, lambda t: [small_3d(t), small_3d(t)], None, float_types),\n",
    "    ('addbmm', small_2d, lambda t: [number(0.4, 2, t), small_3d(t), small_3d(t)], 'scalar'),\n",
    "    ('addbmm', small_2d, lambda t: [number(0.5, 3, t), number(0.4, 2, t), small_3d(t), small_3d(t)], 'two_scalars'),\n",
    "    ('baddbmm', small_3d, lambda t: [small_3d(t), small_3d(t)],),\n",
    "    ('baddbmm', small_3d, lambda t: [number(0.4, 2, t), small_3d(t), small_3d(t)], 'scalar'),\n",
    "    ('baddbmm', small_3d, lambda t: [number(0.5, 3, t), number(0.4, 2, t), small_3d(t), small_3d(t)], 'two_scalars'),\n",
    "    ('addcdiv', small_2d_lapack, lambda t: [small_2d_lapack(t).mul(2), small_2d_lapack(t)],),\n",
    "    ('addcdiv', small_2d_lapack, lambda t: [number(2.8, 1, t),\n",
    "                                            small_2d_lapack(t).mul(2), small_2d_lapack(t)], 'scalar'),\n",
    "    ('addcmul', small_3d, lambda t: [small_3d(t), small_3d(t)],),\n",
    "    ('addcmul', small_3d, lambda t: [number(0.4, 2, t), small_3d(t), small_3d(t)], 'scalar'),\n",
    "    ('addmm', medium_2d, lambda t: [medium_2d(t), medium_2d(t)],),\n",
    "    ('addmm', medium_2d, lambda t: [number(0.4, 2, t), medium_2d(t), medium_2d(t)], 'scalar'),\n",
    "    ('addmm', medium_2d, lambda t: [number(0.5, 3, t), number(0.4, 2, t), medium_2d(t), medium_2d(t)], 'two_scalars'),\n",
    "    ('addmv', medium_1d, lambda t: [medium_2d(t), medium_1d(t)],),\n",
    "    ('addmv', medium_1d, lambda t: [number(0.4, 2, t), medium_2d(t), medium_1d(t)], 'scalar'),\n",
    "    ('addmv', medium_1d, lambda t: [number(0.5, 3, t), number(0.4, 2, t), medium_2d(t), medium_1d(t)], 'two_scalars'),\n",
    "    ('addr', medium_2d, lambda t: [medium_1d(t), medium_1d(t)],),\n",
    "    ('addr', medium_2d, lambda t: [number(0.4, 2, t), medium_1d(t), medium_1d(t)], 'scalar'),\n",
    "    ('addr', medium_2d, lambda t: [number(0.5, 3, t), number(0.4, 2, t), medium_1d(t), medium_1d(t)], 'two_scalars'),\n",
    "    ('atan2', medium_2d, lambda t: [medium_2d(t)], None, float_types),\n",
    "    ('fmod', small_3d, lambda t: [3], 'value'),\n",
    "    ('fmod', small_3d, lambda t: [small_3d_positive(t)], 'tensor'),\n",
    "    ('chunk', medium_2d, lambda t: [4],),\n",
    "    ('chunk', medium_2d, lambda t: [4, 1], 'dim'),\n",
    "    ('chunk', medium_2d, lambda t: [4, -2], 'neg_dim'),\n",
    "    ('clamp', medium_2d_scaled, lambda t: [-1, 5],),\n",
    "    ('clone', medium_2d, lambda t: [],),\n",
    "    ('contiguous', medium_2d, lambda t: [],),\n",
    "    ('cross', new_t(M, 3, M), lambda t: [new_t(M, 3, M)(t)],),\n",
    "    ('cumprod', small_3d, lambda t: [1],),\n",
    "    ('cumprod', small_3d, lambda t: [-1], 'neg_dim'),\n",
    "    ('cumsum', small_3d, lambda t: [1],),\n",
    "    ('cumsum', small_3d, lambda t: [-1], 'neg_dim'),\n",
    "    ('dim', small_3d, lambda t: [],),\n",
    "    ('dist', small_2d, lambda t: [small_2d(t)],),\n",
    "    ('dist', small_2d, lambda t: [small_2d(t), 3], '3_norm'),\n",
    "    ('dist', small_2d, lambda t: [small_2d(t), 2.5], '2_5_norm'),\n",
    "    ('dot', medium_1d, lambda t: [medium_1d(t)],),\n",
    "    ('element_size', medium_1d, lambda t: [],),\n",
    "    ('eq', small_3d_ones, lambda t: [small_3d(t)],),\n",
    "    ('eq', small_3d_ones, lambda t: [small_3d_ones(t)], 'equal'),\n",
    "    ('ne', small_3d_ones, lambda t: [small_3d(t)],),\n",
    "    ('ne', small_3d_ones, lambda t: [small_3d_ones(t)], 'equal'),\n",
    "    ('equal', small_3d_ones, lambda t: [small_3d_ones(t)], 'equal'),\n",
    "    ('equal', small_3d_ones, lambda t: [small_3d(t)],),\n",
    "    ('expand', new_t(M, 1, M), lambda t: [M, 4, M],),\n",
    "    ('expand_as', new_t(M, 1, M), lambda t: [new_t(M, 4, M)(t)],),\n",
    "    ('fill', medium_2d, lambda t: [number(3.14, 3, t)],),\n",
    "    ('ge', medium_2d, lambda t: [medium_2d(t)],),\n",
    "    ('le', medium_2d, lambda t: [medium_2d(t)],),\n",
    "    ('gt', medium_2d, lambda t: [medium_2d(t)],),\n",
    "    ('lt', medium_2d, lambda t: [medium_2d(t)],),\n",
    "    ('is_contiguous', medium_2d, lambda t: [],),\n",
    "    # TODO: can't check negative case - GPU copy will be contiguous\n",
    "    ('is_same_size', medium_2d, lambda t: [small_3d(t)], 'negative'),\n",
    "    ('is_same_size', medium_2d, lambda t: [medium_2d(t)], 'positive'),\n",
    "    ('is_set_to', medium_2d, lambda t: [medium_2d(t)],),\n",
    "    # TODO: positive case\n",
    "    ('kthvalue', small_3d_unique, lambda t: [3],),\n",
    "    ('kthvalue', small_3d_unique, lambda t: [3, 1], 'dim'),\n",
    "    ('kthvalue', small_3d_unique, lambda t: [3, -1], 'neg_dim'),\n",
    "    ('lerp', small_3d, lambda t: [small_3d(t), 0.3],),\n",
    "    ('max', small_3d_unique, lambda t: [],),\n",
    "    ('max', small_3d_unique, lambda t: [1], 'dim'),\n",
    "    ('max', small_3d_unique, lambda t: [-1], 'neg_dim'),\n",
    "    ('max', medium_2d, lambda t: [medium_2d(t)], 'elementwise'),\n",
    "    ('min', small_3d_unique, lambda t: [],),\n",
    "    ('min', small_3d_unique, lambda t: [1], 'dim'),\n",
    "    ('min', small_3d_unique, lambda t: [-1], 'neg_dim'),\n",
    "    ('min', medium_2d, lambda t: [medium_2d(t)], 'elementwise'),\n",
    "    ('mean', small_3d, lambda t: [],),\n",
    "    ('mean', small_3d, lambda t: [-1], 'neg_dim'),\n",
    "    ('mean', small_3d, lambda t: [1], 'dim'),\n",
    "    ('mode', small_3d, lambda t: [],),\n",
    "    ('mode', small_3d, lambda t: [1], 'dim'),\n",
    "    ('mode', small_3d, lambda t: [-1], 'neg_dim'),\n",
    "    ('remainder', small_3d, lambda t: [3], 'value'),\n",
    "    ('remainder', small_3d, lambda t: [-3], 'negative_value'),\n",
    "    ('remainder', small_3d, lambda t: [small_3d_positive(t)], 'tensor'),\n",
    "    ('remainder', small_3d, lambda t: [0 - small_3d_positive(t)], 'negative_tensor'),\n",
    "    ('std', small_3d, lambda t: [],),\n",
    "    ('std', small_3d, lambda t: [1], 'dim'),\n",
    "    ('std', small_3d, lambda t: [-1], 'neg_dim'),\n",
    "    ('var', small_3d, lambda t: [],),\n",
    "    ('var', small_3d, lambda t: [1], 'dim'),\n",
    "    ('var', small_3d, lambda t: [-1], 'neg_dim'),\n",
    "    ('ndimension', small_3d, lambda t: [],),\n",
    "    ('nelement', small_3d, lambda t: [],),\n",
    "    ('numel', small_3d, lambda t: [],),\n",
    "    ('narrow', small_3d, lambda t: [1, 3, 2],),\n",
    "    ('narrow', small_3d, lambda t: [-1, 3, 2], 'neg_dim'),\n",
    "    ('nonzero', small_3d, lambda t: [],),\n",
    "    ('norm', small_3d, lambda t: [],),\n",
    "    ('norm', small_3d, lambda t: [3], '3_norm'),\n",
    "    ('norm', small_3d, lambda t: [3, 0], '3_norm_dim'),\n",
    "    ('norm', small_3d, lambda t: [3, -2], '3_norm_neg_dim'),\n",
    "    ('ones', small_3d, lambda t: [1, 2, 3, 4, 5],),\n",
    "    ('permute', new_t(1, 2, 3, 4), lambda t: [2, 1, 3, 0],),\n",
    "    ('prod', small_2d_oneish, lambda t: [],),\n",
    "    ('prod', small_3d, lambda t: [1], 'dim'),\n",
    "    ('prod', small_3d, lambda t: [-1], 'neg_dim'),\n",
    "    ('sum', small_2d, lambda t: [],),\n",
    "    ('sum', small_3d, lambda t: [1], 'dim'),\n",
    "    ('sum', small_3d, lambda t: [-1], 'neg_dim'),\n",
    "    ('renorm', small_3d, lambda t: [2, 1, 1], '2_norm'),\n",
    "    ('renorm', small_3d, lambda t: [2, -1, 1], '2_norm_neg_dim'),\n",
    "    ('renorm', small_3d, lambda t: [1.5, 1, 1], '1_5_norm'),\n",
    "    ('repeat', small_2d, lambda t: [2, 2, 2],),\n",
    "    ('size', new_t(1, 2, 3, 4), lambda t: [],),\n",
    "    ('size', new_t(1, 2, 3, 4), lambda t: [1], 'dim'),\n",
    "    ('size', new_t(1, 2, 3, 4), lambda t: [-2], 'neg_dim'),\n",
    "    ('sort', small_3d_unique, lambda t: [],),\n",
    "    ('sort', small_3d_unique, lambda t: [1], 'dim'),\n",
    "    ('sort', small_3d_unique, lambda t: [-1], 'neg_dim'),\n",
    "    ('sort', small_3d_unique, lambda t: [1, True], 'dim_descending'),\n",
    "    ('sort', small_3d_unique, lambda t: [-1, True], 'neg_dim_descending'),\n",
    "    ('split', small_3d, lambda t: [2],),\n",
    "    ('split', small_3d, lambda t: [2, 1], 'dim'),\n",
    "    ('split', small_3d, lambda t: [2, -3], 'neg_dim'),\n",
    "    ('squeeze', new_t(1, 2, 1, 4), lambda t: [],),\n",
    "    ('squeeze', new_t(1, 2, 1, 4), lambda t: [2], 'dim'),\n",
    "    ('squeeze', new_t(1, 2, 1, 4), lambda t: [-2], 'neg_dim'),\n",
    "    ('t', new_t(1, 2), lambda t: [],),\n",
    "    ('transpose', new_t(1, 2, 3, 4), lambda t: [1, 2],),\n",
    "    ('transpose', new_t(1, 2, 3, 4), lambda t: [-1, -2], 'neg_dim'),\n",
    "    ('to_list', small_3d, lambda t: [],),\n",
    "    ('topk', small_3d_unique, lambda t: [2, 1, False, True], 'dim_sort'),\n",
    "    ('topk', small_3d_unique, lambda t: [2, -1, False, True], 'neg_dim_sort'),\n",
    "    ('topk', small_3d_unique, lambda t: [2, 1, True, True], 'dim_desc_sort'),\n",
    "    ('trace', medium_2d, lambda t: [],),\n",
    "    ('tril', medium_2d, lambda t: [],),\n",
    "    ('tril', medium_2d, lambda t: [2], 'positive'),\n",
    "    ('tril', medium_2d, lambda t: [-2], 'negative'),\n",
    "    ('triu', medium_2d, lambda t: [],),\n",
    "    ('triu', medium_2d, lambda t: [2], 'positive'),\n",
    "    ('triu', medium_2d, lambda t: [-2], 'negative'),\n",
    "    ('unsqueeze', new_t(2, 3, 4), lambda t: [2],),\n",
    "    ('unsqueeze', new_t(2, 3, 4), lambda t: [-2], 'neg_dim'),\n",
    "    ('view', small_3d, lambda t: [100, 10],),\n",
    "    ('view_as', small_3d, lambda t: [t(100, 10)],),\n",
    "    ('zero', small_3d, lambda t: [],),\n",
    "    ('zeros', small_3d, lambda t: [1, 2, 3, 4],),\n",
    "    ('rsqrt', lambda t: small_3d(t) + 1, lambda t: [], None, float_types),\n",
    "    ('sinh', lambda t: small_3d(t).clamp(-1, 1), lambda t: [], None, float_types),\n",
    "    ('tan', lambda t: small_3d(t).clamp(-1, 1), lambda t: [], None, float_types),\n",
    "    # lapack tests\n",
    "    ('qr', small_2d_lapack, lambda t: [], 'square', float_types),\n",
    "    ('qr', small_2d_lapack_skinny, lambda t: [], 'skinny', float_types),\n",
    "    ('qr', small_2d_lapack_fat, lambda t: [], 'fat', float_types),\n",
    "    ('qr', large_2d_lapack, lambda t: [], 'big', float_types),\n",
    "\n",
    "]\n",
    "\n",
    "# TODO: random functions, cat, gather, scatter, index*, masked*,\n",
    "#       resize, resizeAs, storage_offset, storage, stride, unfold\n",
    "\n",
    "custom_precision = {\n",
    "    'addbmm': 1e-4,\n",
    "    'addmm': 1e-4,\n",
    "    'addmv': 1e-4,\n",
    "    'addr': 1e-4,\n",
    "    'baddbmm': 1e-4,\n",
    "    'rsqrt': 1e-4,\n",
    "    'cumprod': 1e-4,\n",
    "    'qr': 1e-4,\n",
    "}\n",
    "\n",
    "simple_pointwise = [\n",
    "    'abs',\n",
    "    'sign',\n",
    "]\n",
    "for fn in simple_pointwise:\n",
    "    tests.append((fn, small_3d, lambda t: []))\n",
    "\n",
    "simple_pointwise_float = [\n",
    "    'log',\n",
    "    'log1p',\n",
    "    'sigmoid',\n",
    "    'sin',\n",
    "    'sqrt',\n",
    "    'tanh',\n",
    "    'acos',\n",
    "    'asin',\n",
    "    'atan',\n",
    "    'cos',\n",
    "    'cosh',\n",
    "    'exp',\n",
    "    'reciprocal',\n",
    "    'floor',\n",
    "    'frac',\n",
    "    'neg',\n",
    "    'round',\n",
    "    'trunc',\n",
    "    'ceil',\n",
    "]\n",
    "\n",
    "for fn in simple_pointwise_float:\n",
    "    tests.append((fn, small_3d, lambda t: [], None, float_types))\n",
    "\n",
    "_cycles_per_ms = None\n",
    "\n",
    "\n",
    "def get_cycles_per_ms():\n",
    "    \"\"\"Approximate number of cycles per millisecond for torch.cuda._sleep\"\"\"\n",
    "    global _cycles_per_ms\n",
    "    if _cycles_per_ms is None:\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        torch.cuda._sleep(1000000)\n",
    "        end.record()\n",
    "        end.synchronize()\n",
    "        _cycles_per_ms = 1000000 / start.elapsed_time(end)\n",
    "    return _cycles_per_ms\n",
    "\n",
    "\n",
    "def compare_cpu_gpu(tensor_constructor, arg_constructor, fn, t, precision=1e-5):\n",
    "    def tmp(self):\n",
    "        cpu_tensor = tensor_constructor(t)\n",
    "        gpu_tensor = to_gpu(cpu_tensor)\n",
    "        cpu_args = arg_constructor(t)\n",
    "        gpu_args = [to_gpu(arg) for arg in cpu_args]\n",
    "        cpu_result = getattr(cpu_tensor, fn)(*cpu_args)\n",
    "        try:\n",
    "            gpu_result = getattr(gpu_tensor, fn)(*gpu_args)\n",
    "        except RuntimeError as e:\n",
    "            reason = e.args[0]\n",
    "            if 'unimplemented data type' in reason:\n",
    "                raise unittest.SkipTest('unimplemented data type')\n",
    "            raise\n",
    "        except AttributeError as e:\n",
    "            reason = e.args[0]\n",
    "            if 'object has no attribute' in reason:\n",
    "                raise unittest.SkipTest('unimplemented data type')\n",
    "            raise\n",
    "        # If one changes, another should change as well\n",
    "        self.assertEqual(cpu_tensor, gpu_tensor, precision)\n",
    "        self.assertEqual(cpu_args, gpu_args, precision)\n",
    "        # Compare results\n",
    "        self.assertEqual(cpu_result, gpu_result, precision)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "class TestCuda(TestCase):\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"only one GPU detected\")\n",
    "    def test_autogpu(self):\n",
    "        x = torch.randn(5, 5).cuda()\n",
    "        y = torch.randn(5, 5).cuda()\n",
    "        self.assertEqual(x.get_device(), 0)\n",
    "        self.assertEqual(x.get_device(), 0)\n",
    "        with torch.cuda.device(1):\n",
    "            z = torch.randn(5, 5).cuda()\n",
    "            self.assertEqual(z.get_device(), 1)\n",
    "            q = x.add(y)\n",
    "            self.assertEqual(q.get_device(), 0)\n",
    "            w = torch.randn(5, 5).cuda()\n",
    "            self.assertEqual(w.get_device(), 1)\n",
    "        z = z.cuda()\n",
    "        self.assertEqual(z.get_device(), 0)\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"only one GPU detected\")\n",
    "    def test_copy_device(self):\n",
    "        x = torch.randn(5, 5).cuda()\n",
    "        with torch.cuda.device(1):\n",
    "            y = x.cuda()\n",
    "            self.assertEqual(y.get_device(), 1)\n",
    "            self.assertIs(y.cuda(), y)\n",
    "            z = y.cuda(0)\n",
    "            self.assertEqual(z.get_device(), 0)\n",
    "            self.assertIs(z.cuda(0), z)\n",
    "\n",
    "        x = torch.randn(5, 5)\n",
    "        with torch.cuda.device(1):\n",
    "            y = x.cuda()\n",
    "            self.assertEqual(y.get_device(), 1)\n",
    "            self.assertIs(y.cuda(), y)\n",
    "            z = y.cuda(0)\n",
    "            self.assertEqual(z.get_device(), 0)\n",
    "            self.assertIs(z.cuda(0), z)\n",
    "\n",
    "    def test_serialization_array_with_storage(self):\n",
    "        x = torch.randn(5, 5).cuda()\n",
    "        y = torch.IntTensor(2, 5).fill_(0).cuda()\n",
    "        q = [x, y, x, y.storage()]\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            torch.save(q, f)\n",
    "            f.seek(0)\n",
    "            q_copy = torch.load(f)\n",
    "        self.assertEqual(q_copy, q, 0)\n",
    "        q_copy[0].fill_(5)\n",
    "        self.assertEqual(q_copy[0], q_copy[2], 0)\n",
    "        self.assertTrue(isinstance(q_copy[0], torch.cuda.DoubleTensor))\n",
    "        self.assertTrue(isinstance(q_copy[1], torch.cuda.IntTensor))\n",
    "        self.assertTrue(isinstance(q_copy[2], torch.cuda.DoubleTensor))\n",
    "        self.assertTrue(isinstance(q_copy[3], torch.cuda.IntStorage))\n",
    "        q_copy[1].fill_(10)\n",
    "        self.assertTrue(q_copy[3], torch.cuda.IntStorage(10).fill_(10))\n",
    "\n",
    "    def test_type_conversions(self):\n",
    "        x = torch.randn(5, 5)\n",
    "        self.assertIs(type(x.float()), torch.FloatTensor)\n",
    "        self.assertIs(type(x.cuda()), torch.cuda.DoubleTensor)\n",
    "        self.assertIs(type(x.cuda().float()), torch.cuda.FloatTensor)\n",
    "        self.assertIs(type(x.cuda().float().cpu()), torch.FloatTensor)\n",
    "        self.assertIs(type(x.cuda().float().cpu().int()), torch.IntTensor)\n",
    "\n",
    "        y = x.storage()\n",
    "        self.assertIs(type(y.float()), torch.FloatStorage)\n",
    "        self.assertIs(type(y.cuda()), torch.cuda.DoubleStorage)\n",
    "        self.assertIs(type(y.cuda().float()), torch.cuda.FloatStorage)\n",
    "        self.assertIs(type(y.cuda().float().cpu()), torch.FloatStorage)\n",
    "        self.assertIs(type(y.cuda().float().cpu().int()), torch.IntStorage)\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"only one GPU detected\")\n",
    "    def test_type_conversions_same_gpu(self):\n",
    "        x = torch.randn(5, 5).cuda(1)\n",
    "        self.assertEqual(x.int().get_device(), 1)\n",
    "\n",
    "    def _test_broadcast(self, input):\n",
    "        if torch.cuda.device_count() < 2:\n",
    "            raise unittest.SkipTest(\"only one GPU detected\")\n",
    "        result = comm.broadcast(input, (0, 1))\n",
    "        for i, t in enumerate(result):\n",
    "            self.assertEqual(t.get_device(), i)\n",
    "            self.assertEqual(t, input)\n",
    "\n",
    "    def test_broadcast_cpu(self):\n",
    "        self._test_broadcast(torch.randn(5, 5))\n",
    "\n",
    "    def test_broadcast_gpu(self):\n",
    "        self._test_broadcast(torch.randn(5, 5))\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"only one GPU detected\")\n",
    "    def test_broadcast_coalesced(self):\n",
    "        numel = 5\n",
    "        num_bytes = numel * 8\n",
    "        tensors = [\n",
    "            torch.randn(numel).long().cuda(),\n",
    "            torch.randn(numel).cuda(),\n",
    "            torch.randn(numel).long().cuda(),\n",
    "            torch.randn(numel).long().cuda(),\n",
    "            torch.randn(numel * 2).int().cuda(),  # int is 2x shorter\n",
    "            torch.randn(numel).cuda(),\n",
    "        ]\n",
    "\n",
    "        b_tensors = [comm.broadcast(t, (0, 1)) for t in tensors]\n",
    "        for (_, bt), t in zip(b_tensors, tensors):\n",
    "            self.assertEqual(bt.get_device(), 1)\n",
    "            self.assertEqual(bt, t)\n",
    "            self.assertIsInstance(bt, type(t))\n",
    "\n",
    "        bc_tensors = comm.broadcast_coalesced(tensors, (0, 1), buffer_size=num_bytes * 5 // 2)\n",
    "        bc_tensors_t = list(zip(*bc_tensors))\n",
    "        self.assertEqual(b_tensors, bc_tensors_t)\n",
    "        for (_, bt), (_, bct) in zip(b_tensors, bc_tensors_t):\n",
    "            self.assertEqual(bt.get_device(), bct.get_device())\n",
    "            self.assertIsInstance(bct, type(bt))\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"only one GPU detected\")\n",
    "    def test_reduce_add(self):\n",
    "        x = torch.randn(5, 5)\n",
    "        y = torch.randn(5, 5)\n",
    "        x_cuda = x.cuda(0)\n",
    "        y_cuda = y.cuda(1)\n",
    "        result = comm.reduce_add((x_cuda, y_cuda))\n",
    "        self.assertEqual(result.get_device(), 0)\n",
    "        self.assertEqual(result.cpu(), x + y)\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"only one GPU detected\")\n",
    "    def test_reduce_add_coalesced(self):\n",
    "        numel = 5\n",
    "        num_bytes = numel * 8\n",
    "        tensors = [\n",
    "            torch.randn(numel).long().cuda(),\n",
    "            torch.randn(numel).cuda(),\n",
    "            torch.randn(numel).long().cuda(),\n",
    "            torch.randn(numel).long().cuda(),\n",
    "            torch.randn(numel * 2).int().cuda(),  # int is 2x shorter\n",
    "            torch.randn(numel).cuda(),\n",
    "        ]\n",
    "        dup_tensors = [tensors, list(map(lambda t: t.cuda(1), tensors))]\n",
    "\n",
    "        r_tensors = list(map(comm.reduce_add, zip(*dup_tensors)))\n",
    "        for r, t in zip(r_tensors, tensors):\n",
    "            self.assertEqual(r.get_device(), t.get_device())\n",
    "            self.assertEqual(r, t * 2)\n",
    "            self.assertIsInstance(r, type(t))\n",
    "\n",
    "        rc_tensors = comm.reduce_add_coalesced(dup_tensors, buffer_size=num_bytes * 5 // 2)\n",
    "        self.assertEqual(r_tensors, rc_tensors)\n",
    "        for r, rc in zip(r_tensors, rc_tensors):\n",
    "            self.assertEqual(rc.get_device(), r.get_device())\n",
    "            self.assertIsInstance(rc, type(r))\n",
    "\n",
    "    def _test_scatter(self, input, chunk_sizes=None, dim=0):\n",
    "        if torch.cuda.device_count() < 2:\n",
    "            raise unittest.SkipTest(\"only one GPU detected\")\n",
    "        result = comm.scatter(input, (0, 1), chunk_sizes, dim)\n",
    "        self.assertEqual(len(result), 2)\n",
    "        if chunk_sizes is None:\n",
    "            chunk_sizes = tuple(repeat(input.size(dim) // 2, 2))\n",
    "        chunk_start = 0\n",
    "        for i, r in enumerate(result):\n",
    "            chunk_end = chunk_start + chunk_sizes[i]\n",
    "            index = [slice(None, None), slice(None, None)]\n",
    "            index[dim] = slice(chunk_start, chunk_end)\n",
    "            self.assertEqual(r, input[tuple(index)], 0)\n",
    "            chunk_start = chunk_end\n",
    "\n",
    "    def test_scatter_cpu(self):\n",
    "        self._test_scatter(torch.randn(4, 4), dim=0)\n",
    "\n",
    "    def test_scatter_cpu_dim(self):\n",
    "        self._test_scatter(torch.randn(4, 4), dim=1)\n",
    "\n",
    "    def test_scatter_cpu_neg_dim(self):\n",
    "        self._test_scatter(torch.randn(4, 4), dim=-2)\n",
    "\n",
    "    def test_scatter_cpu_sizes(self):\n",
    "        self._test_scatter(torch.randn(6, 4), chunk_sizes=(2, 4))\n",
    "\n",
    "    def test_scatter_gpu(self):\n",
    "        self._test_scatter(torch.randn(4, 4).cuda(), dim=0)\n",
    "\n",
    "    def test_scatter_gpu_dim(self):\n",
    "        self._test_scatter(torch.randn(4, 4).cuda(), dim=1)\n",
    "\n",
    "    def test_scatter_gpu_neg_dim(self):\n",
    "        self._test_scatter(torch.randn(4, 4).cuda(), dim=-2)\n",
    "\n",
    "    def test_scatter_gpu_sizes(self):\n",
    "        self._test_scatter(torch.randn(6, 4).cuda(), chunk_sizes=(2, 4))\n",
    "\n",
    "    def _test_gather(self, dim):\n",
    "        if torch.cuda.device_count() < 2:\n",
    "            raise unittest.SkipTest(\"only one GPU detected\")\n",
    "        x = torch.randn(2, 5).cuda(0)\n",
    "        y = torch.randn(2, 5).cuda(1)\n",
    "        result = comm.gather((x, y), dim)\n",
    "\n",
    "        expected_size = list(x.size())\n",
    "        expected_size[dim] += y.size(dim)\n",
    "        expected_size = torch.Size(expected_size)\n",
    "        self.assertEqual(result.get_device(), 0)\n",
    "        self.assertEqual(result.size(), expected_size)\n",
    "\n",
    "        index = [slice(None, None), slice(None, None)]\n",
    "        index[dim] = slice(0, x.size(dim))\n",
    "        self.assertEqual(result[tuple(index)], x)\n",
    "        index[dim] = slice(x.size(dim), x.size(dim) + y.size(dim))\n",
    "        self.assertEqual(result[tuple(index)], y)\n",
    "\n",
    "    def test_gather(self):\n",
    "        self._test_gather(0)\n",
    "\n",
    "    def test_gather_dim(self):\n",
    "        self._test_gather(1)\n",
    "\n",
    "    def test_from_sequence(self):\n",
    "        seq = [list(range(i * 4, i * 4 + 4)) for i in range(5)]\n",
    "        reference = torch.arange(0, 20).resize_(5, 4)\n",
    "        for t in types:\n",
    "            cuda_type = get_gpu_type(t)\n",
    "            self.assertEqual(cuda_type(seq), reference)\n",
    "\n",
    "    def test_manual_seed(self):\n",
    "        with freeze_rng_state():\n",
    "            x = torch.zeros(4, 4).float().cuda()\n",
    "            torch.cuda.manual_seed(2)\n",
    "            self.assertEqual(torch.cuda.initial_seed(), 2)\n",
    "            x.uniform_()\n",
    "            torch.cuda.manual_seed(2)\n",
    "            y = x.clone().uniform_()\n",
    "            self.assertEqual(x, y)\n",
    "            self.assertEqual(torch.cuda.initial_seed(), 2)\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"only one GPU detected\")\n",
    "    def test_cat_autogpu(self):\n",
    "        x = torch.randn(4, 4).cuda(1)\n",
    "        y = torch.randn(4, 4).cuda(1)\n",
    "        z = torch.cat([x, y], 0)\n",
    "        self.assertEqual(z.get_device(), x.get_device())\n",
    "\n",
    "    def test_serialization(self):\n",
    "        x = torch.randn(4, 4).cuda()\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            torch.save(x, f)\n",
    "            f.seek(0)\n",
    "            x_copy = torch.load(f)\n",
    "        self.assertEqual(x_copy, x)\n",
    "        self.assertIs(type(x_copy), type(x))\n",
    "        self.assertEqual(x_copy.get_device(), x.get_device())\n",
    "\n",
    "    def test_serialization_array_with_empty(self):\n",
    "        x = [torch.randn(4, 4).cuda(), torch.cuda.FloatTensor()]\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            torch.save(x, f)\n",
    "            f.seek(0)\n",
    "            x_copy = torch.load(f)\n",
    "        for original, copy in zip(x, x_copy):\n",
    "            self.assertEqual(copy, original)\n",
    "            self.assertIs(type(copy), type(original))\n",
    "            self.assertEqual(copy.get_device(), original.get_device())\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"detected only one GPU\")\n",
    "    def test_multigpu_serialization(self):\n",
    "        x = [torch.randn(4, 4).cuda(0), torch.randn(4, 4).cuda(1)]\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            torch.save(x, f)\n",
    "            f.seek(0)\n",
    "            x_copy = torch.load(f)\n",
    "        for original, copy in zip(x, x_copy):\n",
    "            self.assertEqual(copy, original)\n",
    "            self.assertIs(type(copy), type(original))\n",
    "            self.assertEqual(copy.get_device(), original.get_device())\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"detected only one GPU\")\n",
    "    def test_multigpu_serialization_remap(self):\n",
    "        x = [torch.randn(4, 4).cuda(0), torch.randn(4, 4).cuda(1)]\n",
    "\n",
    "        def gpu_remap(storage, location):\n",
    "            if location == 'cuda:1':\n",
    "                return storage.cuda(0)\n",
    "\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            torch.save(x, f)\n",
    "            f.seek(0)\n",
    "            x_copy = torch.load(f, map_location=gpu_remap)\n",
    "\n",
    "        for original, copy in zip(x, x_copy):\n",
    "            self.assertEqual(copy, original)\n",
    "            self.assertIs(type(copy), type(original))\n",
    "            self.assertEqual(copy.get_device(), 0)\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"detected only one GPU\")\n",
    "    def test_multigpu_serialization_remap_dict(self):\n",
    "        x = [torch.randn(4, 4).cuda(0), torch.randn(4, 4).cuda(1)]\n",
    "        with tempfile.NamedTemporaryFile() as f:\n",
    "            torch.save(x, f)\n",
    "            f.seek(0)\n",
    "            x_copy = torch.load(f, map_location={'cuda:1': 'cuda:0'})\n",
    "        for original, copy in zip(x, x_copy):\n",
    "            self.assertEqual(copy, original)\n",
    "            self.assertIs(type(copy), type(original))\n",
    "            self.assertEqual(copy.get_device(), 0)\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"detected only one GPU\")\n",
    "    def test_cuda_set_device(self):\n",
    "        x = torch.randn(5, 5)\n",
    "        with torch.cuda.device(1):\n",
    "            self.assertEqual(x.cuda().get_device(), 1)\n",
    "            torch.cuda.set_device(0)\n",
    "            self.assertEqual(x.cuda().get_device(), 0)\n",
    "            with torch.cuda.device(1):\n",
    "                self.assertEqual(x.cuda().get_device(), 1)\n",
    "            self.assertEqual(x.cuda().get_device(), 0)\n",
    "            torch.cuda.set_device(1)\n",
    "        self.assertEqual(x.cuda().get_device(), 0)\n",
    "\n",
    "    def test_is_tensor(self):\n",
    "        for t in types:\n",
    "            tensor = get_gpu_type(t)()\n",
    "            self.assertTrue(torch.is_tensor(tensor))\n",
    "        self.assertTrue(torch.is_tensor(torch.cuda.HalfTensor()))\n",
    "\n",
    "    def test_cuda_synchronize(self):\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    def test_streams(self):\n",
    "        default_stream = torch.cuda.current_stream()\n",
    "        user_stream = torch.cuda.Stream()\n",
    "        self.assertEqual(torch.cuda.current_stream(), default_stream)\n",
    "        self.assertNotEqual(default_stream, user_stream)\n",
    "        self.assertEqual(default_stream.cuda_stream, 0)\n",
    "        self.assertNotEqual(user_stream.cuda_stream, 0)\n",
    "        with torch.cuda.stream(user_stream):\n",
    "            self.assertEqual(torch.cuda.current_stream(), user_stream)\n",
    "        self.assertTrue(user_stream.query())\n",
    "        # copy 10 MB tensor from CPU-GPU which should take some time\n",
    "        tensor1 = torch.ByteTensor(10000000).pin_memory()\n",
    "        tensor2 = tensor1.cuda(async=True)\n",
    "        self.assertFalse(default_stream.query())\n",
    "        default_stream.synchronize()\n",
    "        self.assertTrue(default_stream.query())\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"detected only one GPU\")\n",
    "    def test_streams_multi_gpu(self):\n",
    "        default_stream = torch.cuda.current_stream()\n",
    "        self.assertEqual(default_stream.device, 0)\n",
    "        stream = torch.cuda.Stream(device=1)\n",
    "        self.assertEqual(stream.device, 1)\n",
    "        with torch.cuda.device(1):\n",
    "            self.assertEqual(torch.cuda.current_stream().device, 1)\n",
    "            self.assertNotEqual(torch.cuda.current_stream(), default_stream)\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"multi-GPU not supported\")\n",
    "    def test_tensor_device(self):\n",
    "        self.assertEqual(torch.cuda.FloatTensor(1).get_device(), 0)\n",
    "        self.assertEqual(torch.cuda.FloatTensor(1, device=1).get_device(), 1)\n",
    "        with torch.cuda.device(1):\n",
    "            self.assertEqual(torch.cuda.FloatTensor(1).get_device(), 1)\n",
    "            self.assertEqual(torch.cuda.FloatTensor(1, device=0).get_device(), 0)\n",
    "            self.assertEqual(torch.cuda.FloatTensor(1, device=None).get_device(), 1)\n",
    "\n",
    "    def test_events(self):\n",
    "        stream = torch.cuda.current_stream()\n",
    "        event = torch.cuda.Event(enable_timing=True)\n",
    "        self.assertTrue(event.query())\n",
    "        start_event = torch.cuda.Event(enable_timing=True)\n",
    "        stream.record_event(start_event)\n",
    "        torch.cuda._sleep(int(50 * get_cycles_per_ms()))\n",
    "        stream.record_event(event)\n",
    "        self.assertFalse(event.query())\n",
    "        event.synchronize()\n",
    "        self.assertTrue(event.query())\n",
    "        self.assertGreater(start_event.elapsed_time(event), 0)\n",
    "\n",
    "    def test_record_stream(self):\n",
    "        cycles_per_ms = get_cycles_per_ms()\n",
    "\n",
    "        t = torch.FloatTensor([1, 2, 3, 4]).pin_memory()\n",
    "        result = torch.cuda.FloatTensor(t.size())\n",
    "        stream = torch.cuda.Stream()\n",
    "        ptr = [None]\n",
    "\n",
    "        # Performs the CPU->GPU copy in a background stream\n",
    "        def perform_copy():\n",
    "            with torch.cuda.stream(stream):\n",
    "                tmp = t.cuda(async=True)\n",
    "                ptr[0] = tmp.data_ptr()\n",
    "            torch.cuda.current_stream().wait_stream(stream)\n",
    "            tmp.record_stream(torch.cuda.current_stream())\n",
    "            torch.cuda._sleep(int(50 * cycles_per_ms))  # delay the copy\n",
    "            result.copy_(tmp)\n",
    "\n",
    "        perform_copy()\n",
    "        with torch.cuda.stream(stream):\n",
    "            tmp2 = torch.cuda.FloatTensor(t.size())\n",
    "            tmp2.zero_()\n",
    "            self.assertNotEqual(tmp2.data_ptr(), ptr[0], 'allocation re-used to soon')\n",
    "\n",
    "        self.assertEqual(result.tolist(), [1, 2, 3, 4])\n",
    "\n",
    "        # Check that the block will be re-used after the main stream finishes\n",
    "        torch.cuda.current_stream().synchronize()\n",
    "        with torch.cuda.stream(stream):\n",
    "            tmp3 = torch.cuda.FloatTensor(t.size())\n",
    "            self.assertEqual(tmp3.data_ptr(), ptr[0], 'allocation not re-used')\n",
    "\n",
    "    def test_caching_pinned_memory(self):\n",
    "        cycles_per_ms = get_cycles_per_ms()\n",
    "\n",
    "        # check that allocations are re-used after deletion\n",
    "        t = torch.FloatTensor([1]).pin_memory()\n",
    "        ptr = t.data_ptr()\n",
    "        del t\n",
    "        t = torch.FloatTensor([1]).pin_memory()\n",
    "        self.assertEqual(t.data_ptr(), ptr, 'allocation not reused')\n",
    "\n",
    "        # check that the allocation is not re-used if it's in-use by a copy\n",
    "        gpu_tensor = torch.cuda.FloatTensor([0])\n",
    "        torch.cuda._sleep(int(50 * cycles_per_ms))  # delay the copy\n",
    "        gpu_tensor.copy_(t, async=True)\n",
    "        del t\n",
    "        t = torch.FloatTensor([1]).pin_memory()\n",
    "        self.assertNotEqual(t.data_ptr(), ptr, 'allocation re-used too soon')\n",
    "        self.assertEqual(list(gpu_tensor), [1])\n",
    "\n",
    "    @unittest.skipIf(torch.cuda.device_count() < 2, \"only one GPU detected\")\n",
    "    def test_caching_pinned_memory_multi_gpu(self):\n",
    "        # checks that the events preventing pinned memory from being re-used\n",
    "        # too early are recorded on the correct GPU\n",
    "        cycles_per_ms = get_cycles_per_ms()\n",
    "\n",
    "        t = torch.FloatTensor([1]).pin_memory()\n",
    "        ptr = t.data_ptr()\n",
    "        gpu_tensor0 = torch.cuda.FloatTensor([0], device=0)\n",
    "        gpu_tensor1 = torch.cuda.FloatTensor([0], device=1)\n",
    "\n",
    "        with torch.cuda.device(1):\n",
    "            torch.cuda._sleep(int(50 * cycles_per_ms))  # delay the copy\n",
    "            gpu_tensor1.copy_(t, async=True)\n",
    "\n",
    "        del t\n",
    "        t = torch.FloatTensor([2]).pin_memory()\n",
    "        self.assertNotEqual(t.data_ptr(), ptr, 'allocation re-used too soon')\n",
    "\n",
    "        with torch.cuda.device(0):\n",
    "            gpu_tensor0.copy_(t, async=True)\n",
    "\n",
    "        self.assertEqual(gpu_tensor1[0], 1)\n",
    "        self.assertEqual(gpu_tensor0[0], 2)\n",
    "\n",
    "    def test_btrifact(self):\n",
    "        TestTorch._test_btrifact(self, lambda t: t.cuda())\n",
    "\n",
    "    def test_btrisolve(self):\n",
    "        TestTorch._test_btrisolve(self, lambda t: t.cuda())\n",
    "\n",
    "    def test_tensor_gather(self):\n",
    "        TestTorch._test_gather(self, lambda t: t.cuda(), False)\n",
    "\n",
    "    def test_tensor_scatter(self):\n",
    "        TestTorch._test_scatter_base(self, lambda t: t.cuda(), 'scatter_', test_bounds=False)\n",
    "\n",
    "    def test_tensor_scatterAdd(self):\n",
    "        TestTorch._test_scatter_base(self, lambda t: t.cuda(), 'scatter_add_', test_bounds=False)\n",
    "\n",
    "    def test_tensor_scatterFill(self):\n",
    "        TestTorch._test_scatter_base(self, lambda t: t.cuda(), 'scatter_', True, test_bounds=False)\n",
    "\n",
    "\n",
    "if HAS_CUDA:\n",
    "    for decl in tests:\n",
    "        for t in types:\n",
    "            tensor = t()\n",
    "            gpu_tensor = get_gpu_type(t)()\n",
    "            if len(decl) == 3:\n",
    "                name, constr, arg_constr = decl\n",
    "                desc = ''\n",
    "            elif len(decl) == 4:\n",
    "                name, constr, arg_constr, desc = decl\n",
    "            elif len(decl) == 5:\n",
    "                name, constr, arg_constr, desc, type_subset = decl\n",
    "                if t not in type_subset:\n",
    "                    continue\n",
    "\n",
    "            precision = custom_precision.get(name, TestCuda.precision)\n",
    "            for inplace in (True, False):\n",
    "                if inplace:\n",
    "                    name_inner = name + '_'\n",
    "                else:\n",
    "                    name_inner = name\n",
    "                if not hasattr(tensor, name_inner):\n",
    "                    continue\n",
    "                if not hasattr(gpu_tensor, name_inner):\n",
    "                    print(\"Ignoring {}, because it's not implemented by torch.cuda.{}\".format(\n",
    "                        name_inner, gpu_tensor.__class__.__name__))\n",
    "                    continue\n",
    "\n",
    "                test_name = 'test_' + t.__name__ + '_' + name_inner\n",
    "                if desc:\n",
    "                    test_name += '_' + desc\n",
    "\n",
    "                assert not hasattr(TestCuda, test_name), \"Duplicated test name: \" + test_name\n",
    "                setattr(TestCuda, test_name, compare_cpu_gpu(constr, arg_constr, name_inner, t, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
